# src/LexaLCM/Config/Pretrain/Config_Pretrain.yaml

training:
  num_denoising_steps: 100 # The denoiser tower runs 100 times, (the contextualizer tower runs only once).
  max_steps: 250000
  warmup_steps: 500
  optimizer: "adafactor"
  max_grad_norm: None # If optimizer is adafactor, this will be ignored
  clip_threshold: 1.0 # If optimizer is not adafactor, this will be ignored
  eval_every: 0
  batch_size: 16 
  learning_rate: None # Adafactor will use the default learning rate
  adafactor_rel_step: True
  adafactor_warmup_init: True
  weight_decay: 0.01
  bf16: true
  max_seq_len: 64  # Truncate sequences that are longer than this *after* adding SoT and EoT
  output_dir: ./outputs
  save_every: 5000
  resume_from: None # Set to True or specify a specific filename to resume from a checkpoint (e.g. ./checkpoints/checkpoint_step_5000.pt), or None to start from scratch
  num_workers: 20 # How many cpu cores to use for data loading
  grad_norm_log_every: 20 # How often to compute and log gradient norms (0 = never, matches logging_steps by default)

gpus: # GPU IDs for each piece of the model's architecture
  custom_multi_gpu: true # If true, we're using custom-multi-GPU where we move pieces of the model to the appropriate devices manually, otherwise we're using DataParallel
  denoiser: 0
  contextualizer: 1
  other: 1

data:
  data_dir: /home/lexa/DevProjects/_Data/LexaLCM_Datasets/src/Datasets/Wikipedia_Ja/
  text_column: text_sentences_sonar_emb
  train_split: Train
  val_split: Val

wandb:
  project: LexaLCM_Pre3
  run_name: lcm_run_001
  log_every: 20
