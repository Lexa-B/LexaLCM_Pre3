# src/LexaLCM/Config/Pretrain/Config_Pretrain_Pre2_001.yaml

training:
  num_denoising_steps: 100 # The denoiser tower runs 100 times, (the contextualizer tower runs only once).
  max_steps: 250000
  warmup_steps: 500
  optimizer: "adafactor"
  max_grad_norm: None # If optimizer is adafactor, this will be ignored
  clip_threshold: 1.0 # If optimizer is not adafactor, this will be ignored
  eval_every: 0
  batch_size: 16 
  learning_rate: None # Adafactor will use the default learning rate
  adafactor_rel_step: True
  adafactor_warmup_init: True
  weight_decay: 0.01
  bf16: true
  max_seq_len: 128  # Truncate sequences that are longer than this *after* adding SoT and EoT
  output_dir: ./outputs
  save_every: 5000
  resume_from: None # Set to True or specify a specific filename to resume from a checkpoint (e.g. ./checkpoints/checkpoint_step_5000.pt), or None to start from scratch
  num_workers: 20 # How many cpu cores to use for data loading

data:
  data_dir: /home/lexa/DevProjects/_Data/LexaLCM_Datasets/src/Datasets/Wikipedia_Ja/
  text_column: text_sentences_sonar_emb
  train_split: Train
  val_split: Val

wandb:
  project: LexaLCM_Pre2
  run_name: lcm_run_001
  log_every: 5
